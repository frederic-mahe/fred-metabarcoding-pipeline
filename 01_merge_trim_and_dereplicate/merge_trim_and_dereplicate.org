* Merge, trim and dereplicate pairs of fastq files

Initial situation: fastq files are already duplicated, we have a pair
of R1 and R2 files for each sample.

To adapt it to another dataset, you need to change primer sequences in
the initial block of variables, and the raw fastq file search pattern
and sample file naming if your raw fastq files follow another naming
rule (in the final =while= loop):

#+BEGIN_SRC sh
  export LC_ALL=C

  # Define variables, temporary files and output files
  declare -r PRIMER_F="TTGTACACACCGCCC"
  declare -r PRIMER_R="CCTTCNGCAGGTTCACCTAC"
  declare -r ANTI_PRIMER_R="GTAGGTGAACCTGCNGAAGG"
  declare -ri THREADS=4
  declare -r CUTADAPT="$(which cutadapt) --minimum-length 32 --cores=${THREADS} --discard-untrimmed --times=2"  # cutadapt 3.1
  declare -r SWARM="$(which swarm)"
  declare -r VSEARCH="$(which vsearch) --quiet"
  declare -ri ENCODING=33
  declare -r MIN_F=$(( ${#PRIMER_F} * 2 / 3 ))  # match is >= 2/3 of primer length
  declare -r MIN_R=$(( ${#PRIMER_R} * 2 / 3 ))
  declare -r FIFOS=$(echo fifo_{merged,trimmed}_fastq fifo_filtered_fasta{,_bis})
  declare -i TICKER=0

  merge_fastq_pair() {
      ${VSEARCH} \
          --threads "${THREADS}" \
          --fastq_mergepairs "${FORWARD}" \
          --reverse "${REVERSE}" \
          --fastq_ascii "${ENCODING}" \
          --fastq_allowmergestagger \
          --fastqout fifo_merged_fastq 2> "${SAMPLE}.log" &
  }

  trim_primers() {
      # search forward primer in both normal and revcomp, from now on
      # all reads are in the same orientation
      ${CUTADAPT} \
          --revcomp \
          -g "${PRIMER_F}" \
          -O "${MIN_F}" fifo_merged_fastq 2>> "${LOG}" | \
          ${CUTADAPT} \
              -a "${ANTI_PRIMER_R}" \
              -O "${MIN_R}" - > fifo_trimmed_fastq 2>> "${LOG}" &
  }

  extract_error_rate() {
      # extract "expected error" annotations (ee) for later quality
      # filtering
      paste - - < fifo_filtered_fasta_bis | \
          awk -F "[>;=\t]" '{print $2, $4, length($NF)}' | \
          sort -k3,3n -k1,1d -k2,2n | \
          uniq --check-chars=40 > "${QUAL}" &
  }

  convert_fastq_to_fasta() {
      # discard reads with Ns, extract "expected error" annotations (ee)
      # for later quality filtering
      ${VSEARCH} \
          --fastq_filter fifo_trimmed_fastq \
          --fastq_maxns 0 \
          --relabel_sha1 \
          --fastq_ascii "${ENCODING}" \
          --eeout \
          --fasta_width 0 \
          --fastaout - 2>> "${LOG}" | \
          tee fifo_filtered_fasta_bis > fifo_filtered_fasta &
  }

  dereplicate_fasta() {
      # discard "expected error" annotations (ee)
      ${VSEARCH} \
          --derep_fulllength fifo_filtered_fasta \
          --sizeout \
          --fasta_width 0 \
          --xee \
          --output "${FASTA}" 2>> "${LOG}"
  }

  list_local_clusters() {
      # retain only clusters with more than 2 reads
      # (do not use the fastidious option here)
      ${SWARM} \
          --threads "${THREADS}" \
          --differences 1 \
          --usearch-abundance \
          --log /dev/null \
          --output-file /dev/null \
          --statistics-file - \
          "${FASTA}" | \
          awk 'BEGIN {FS = OFS = "\t"} $2 > 2' > "${LOCAL_STATS}"
  }

  # from raw fastq files to cooked fasta
  find . -name "*_R1.fastq.gz" -type f -print0 | \
      while IFS= read -r -d '' FORWARD ; do
          TICKER=$(( $TICKER + 1 ))
          echo -e "${TICKER}\t${FORWARD}"
          REVERSE="${FORWARD/_R1/_R2}"
          SAMPLE="${FORWARD/_R1.*/}"
          LOG="${SAMPLE}.log"
          FASTA="${SAMPLE}.fas"
          QUAL="${SAMPLE}.qual"
          LOCAL_STATS="${SAMPLE}.stats"

          # clean (remove older files)
          rm -f "${SAMPLE}".{fas,qual,log,stats} ${FIFOS}
          mkfifo ${FIFOS}

          merge_fastq_pair
          trim_primers
          convert_fastq_to_fasta &
          extract_error_rate
          dereplicate_fasta
          list_local_clusters

          # clean (make sure fifos are not reused)
          rm ${FIFOS}
      done
#+END_SRC

The code above uses named pipes (fifo) to avoid writing intermediate
results to mass storage. The goal is to speed up processing, and to
make the code more modular and clearer.

Under certain rare multithreading conditions, vearch (merge) can hang,
interrupting the data flow and the pipeline. Until that bug can be
reproduced and fixed, be cautious.
